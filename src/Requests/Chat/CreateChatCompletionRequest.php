<?php

declare(strict_types=1);

namespace PhpClient\OpenAI\Requests\Chat;

use Saloon\Contracts\Body\HasBody;
use Saloon\Enums\Method;
use Saloon\Http\Request;
use Saloon\Traits\Body\HasJsonBody;

use function array_filter;

/**
 * Creates a model response for the given chat conversation.
 *
 * @see https://platform.openai.com/docs/api-reference/chat/create
 * @version Relevant for 2025-02-13, OpenAI API v1
 */
final class CreateChatCompletionRequest extends Request implements HasBody
{
    use HasJsonBody;

    protected Method $method = Method::POST;

    /**
     * @param  string  $model  ID of the model to use.
     *
     * @param  array  $messages  A list of messages comprising the conversation so far. Depending on the model you use,
     * different message types (modalities) are supported, like text, images, and audio.
     *
     * @param  bool|null  $store  Whether or not to store the output of this chat completion request for use in our
     * model distillation or evals products.
     *
     * @param  string|null  $reasoningEffort  Constrains effort on reasoning for reasoning models. Currently supported
     * values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens
     * used on reasoning in a response.
     *
     * @param  array|null  $metadata  Set of 16 key-value pairs that can be attached to an object. This can be useful
     * for storing additional information about the object in a structured format, and querying for objects via API
     * or the dashboard.
     *
     * Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of
     * 512 characters.
     *
     * @param  float|null  $frequencyPenalty  Number between -2.0 and 2.0. Positive values penalize new tokens based
     * on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line
     * verbatim.
     *
     * @param  array|null  $logitBias  Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias
     * value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling.
     * The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood
     * of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     *
     * @param  bool|null  $logprobs  Whether to return log probabilities of the output tokens or not. If true, returns
     * the log probabilities of each output token returned in the content of message.
     *
     * @param  int|null  $topLogprobs  An integer between 0 and 20 specifying the number of most likely tokens to
     * return at each token position, each with an associated log probability. logprobs must be set to true if this
     * parameter is used.
     *
     * @param  int|null  $maxCompletionTokens  An upper bound for the number of tokens that can be generated for
     * a completion, including visible output tokens and reasoning tokens.
     *
     * @param  int|null  $n  How many chat completion choices to generate for each input message. Note that you will
     * be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
     *
     * @param  array|null  $modalities  Output types that you would like the model to generate for this request.
     * Most models are capable of generating text, which is the default:
     *
     * ["text"]
     *
     * The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both
     * text and audio responses, you can use:
     *
     * ["text", "audio"]
     *
     * @param  array|null  $prediction  Configuration for a Predicted Output, which can greatly improve response
     * times when large parts of the model response are known ahead of time. This is most common when you are
     * regenerating a file with only minor changes to most of the content.
     *
     * @param  array|null  $audio  Parameters for audio output. Required when audio output is requested with
     * modalities: ["audio"].
     *
     * @param  float|null  $presencePenalty  Number between -2.0 and 2.0. Positive values penalize new tokens based
     * on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
     *
     * @param  array|null  $responseFormat  An object specifying the format that the model must output.
     *
     * Setting to { "type": "json_schema", "json_schema": {...} } enables Structured Outputs which ensures the model
     * will match your supplied JSON schema. Learn more in the Structured Outputs guide.
     *
     * Setting to { "type": "json_object" } enables JSON mode, which ensures the message the model generates
     * is valid JSON.
     *
     * Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or
     * user message. Without this, the model may generate an unending stream of whitespace until the generation reaches
     * the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content
     * may be partially cut off if finish_reason="length", which indicates the generation exceeded max_tokens or
     * the conversation exceeded the max context length.
     *
     * @param  int|null  $seed  This feature is in Beta. If specified, our system will make a best effort to sample
     * deterministically, such that repeated requests with the same seed and parameters should return the same result.
     * Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor
     * changes in the backend.
     *
     * @param  string|null  $serviceTier  Specifies the latency tier to use for processing the request. This parameter
     * is relevant for customers subscribed to the scale tier service:
     *
     * - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they
     * are exhausted.
     * - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default
     * service tier with a lower uptime SLA and no latency guarantee.
     * - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA
     * and no latency guarantee.
     * - When not set, the default behavior is 'auto'.
     *
     * @param  array|string|null  $stop  Up to 4 sequences where the API will stop generating further tokens.
     *
     * @param  bool|null  $stream  If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent
     * as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
     *
     * @param  array|null  $streamOptions  Options for streaming response. Only set this when you set stream: true.
     *
     * @param  float|null  $temperature  What sampling temperature to use, between 0 and 2. Higher values like 0.8
     * will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     * We generally recommend altering this or top_p but not both.
     *
     * @param  float|null  $topP  An alternative to sampling with temperature, called nucleus sampling, where the model
     * considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
     * the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     *
     * @param  array|null  $tools  A list of tools the model may call. Currently, only functions are supported as
     * a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions
     * are supported.
     *
     * @param  array|string|null  $toolChoice  Controls which (if any) tool is called by the model. none means the
     * model will not call any tool and instead generates a message. auto means the model can pick between generating
     * a message or calling one or more tools. required means the model must call one or more tools. Specifying
     * a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call
     * that tool.
     *
     * `none` is the default when no tools are present. `auto` is the default if tools are present.
     *
     * @param  bool|null  $parallelToolCalls  Whether to enable parallel function calling during tool use.
     *
     * @param  string|null  $user  A unique identifier representing your end-user, which can help OpenAI to monitor
     * and detect abuse.
     */
    public function __construct(
        public readonly string $model,
        public readonly array $messages,
        public readonly null|bool $store = null,
        public readonly null|string $reasoningEffort = null,
        public readonly null|array $metadata = null,
        public readonly null|float $frequencyPenalty = null,
        public readonly null|array $logitBias = null,
        public readonly null|bool $logprobs = null,
        public readonly null|int $topLogprobs = null,
        public readonly null|int $maxCompletionTokens = null,
        public readonly null|int $n = null,
        public readonly null|array $modalities = null,
        public readonly null|array $prediction = null,
        public readonly null|array $audio = null,
        public readonly null|float $presencePenalty = null,
        public readonly null|array $responseFormat = null,
        public readonly null|int $seed = null,
        public readonly null|string $serviceTier = null,
        public readonly null|array|string $stop = null,
        public readonly null|bool $stream = null,
        public readonly null|array $streamOptions = null,
        public readonly null|float $temperature = null,
        public readonly null|float $topP = null,
        public readonly null|array $tools = null,
        public readonly null|array|string $toolChoice = null,
        public readonly null|bool $parallelToolCalls = null,
        public readonly null|string $user = null,
    ) {}

    public function resolveEndpoint(): string
    {
        return '/v1/chat/completions';
    }

    protected function defaultBody(): array
    {
        return array_filter(
            array: [
                'model' => $this->model,
                'messages' => $this->messages,
                'store' => $this->store,
                'reasoning_effort' => $this->reasoningEffort,
                'metadata' => $this->metadata,
                'frequency_penalty' => $this->frequencyPenalty,
                'logit_bias' => $this->logitBias,
                'logprobs' => $this->logprobs,
                'top_logprobs' => $this->topLogprobs,
                'max_completion_tokens' => $this->maxCompletionTokens,
                'n' => $this->n,
                'modalities' => $this->modalities,
                'prediction' => $this->prediction,
                'audio' => $this->audio,
                'presence_penalty' => $this->presencePenalty,
                'response_format' => $this->responseFormat,
                'seed' => $this->seed,
                'service_tier' => $this->serviceTier,
                'stop' => $this->stop,
                'stream' => $this->stream,
                'stream_options' => $this->streamOptions,
                'temperature' => $this->temperature,
                'top_p' => $this->topP,
                'tools' => $this->tools,
                'tool_choice' => $this->toolChoice,
                'parallel_tool_calls' => $this->parallelToolCalls,
                'user' => $this->user,
            ],
            callback: static fn(mixed $value): bool => $value !== null,
        );
    }
}
